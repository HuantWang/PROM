{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Enhancing Deployment-time Predictive Model Robustness for Code Analysis and Optimization: Artifacts Evaluation Instructions\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "This interactive Jupyter notebook provides a small-scale demo for showcasing task definition, client RL search, client RL parameter tuning, and evaluation of case studies reported in the paper.\n",
    "\n",
    "The main results of our CGO 2025 paper apply Prom to 5 case studies to detect their drifting samples. The evaluation presented in our paper ran on a much larger dataset and for longer. This notebook contains minimal working examples designed to be evaluated in a reasonable amount of time (approximately 20 minutes).\n",
    "\n",
    "## Instructions for Experimental Workflow:\n",
    "\n",
    "Before you start, please first make a copy of the notebook by going to the landing page. Then select the checkbox next to the notebook titled *main.ipynb*, then click \"**Duplicate**\".\n",
    "\n",
    "Click the name of the newly created Jupyter Notebook, e.g. **AE-Copy1.ipynb**. Next, select \"**Kernel**\" > \"**Restart & Clear Output**\". Then, repeatedly press the play button (the tooltip is \"run cell, select below\") to step through each cell of the notebook.\n",
    "\n",
    "Alternatively, select each cell in turn and use \"**Cell**\"> \"**Run Cell**\" from the menu to run specific cells. Note that some cells depend on previous cells being executed. If any errors occur, ensure all previous cells have been executed.\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "**Some cells can take more than 30 minutes to complete; please wait for the results until step to the next cell.**\n",
    "\n",
    "High load can lead to a long wait for results. This may occur if multiple reviewers are simultaneously trying to generate results.\n",
    "\n",
    "The experiments are customisable as the code provided in the Jupyter Notebook can be edited on the spot. Simply type your changes into the code blocks and re-run using **Cell > Run Cells** from the menu.\n",
    "\n",
    "## Links to The Paper\n",
    "\n",
    "For each step, we note the section number of the submitted version where the relevant technique is described or data is presented.\n",
    "\n",
    "The main results are presented in Figures 7-10 and Table 2 and 3 of the submitted paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 1: Tutorial for Prom\n",
    "\n",
    "This demo corresponds to the simplified drifting detection example given in Figure 2. Note that we have refactored the code; hence there are small changes in the API. This is a small-scale demo for case study 1 of thread coarsening. The full-scale evaluation used in the paper takes over 24 hours to run.\n",
    "\n",
    "## Step 1. Train the underlying model\n",
    "\n",
    "This problem develops a model to determine the optimal OpenCL GPU thread coarsening factor for performance optimization. Following the original paper, an ML model predicts a coarsening factor (ranging  from 1 to 32) for a test OpenCL kernel, where 1 indicates no coarsening. Following the setup of  the DeepTune, we train and test the models using the labeled dataset  from them, comprising 17 OpenCL kernels from three benchmark  suites across four GPU platforms.\n",
    "\n",
    "we train the baseline model using  leave-one-out cross-validation, which involves training the baseline model on 16 OpenCL kernels and testing on another one.\n",
    "#### Training dataset and calibration dataset partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 'C1,C2,C4' activated.\n",
      "Starting to partition the training and calibration datasets...\n",
      "Data splitting process completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import subprocess\n",
    "# setting the environments\n",
    "subprocess.run([\"bash\", \"env.sh\", \"C1\"])\n",
    "\n",
    "import sys\n",
    "sys.path.append('../case_study/Thread/')\n",
    "\n",
    "from Magni_utils import ThreadCoarseningMa,Magni,make_predictionMa,make_prediction_ilMa\n",
    "from Thread_magni import load_magni_args_notebook,load_magni_args\n",
    "from src.prom.prom_util import Prom_utils\n",
    "\n",
    "print(\"Starting to partition the training and calibration datasets...\")\n",
    "\n",
    "# Load necessary arguments for the program, related to model configuration or runtime settings\n",
    "# args = load_magni_args_notebook()\n",
    "args = load_magni_args()\n",
    "seed_value = int(args.seed)\n",
    "# Initialize a thread coarsening model object for the Magni model\n",
    "prom_thread = ThreadCoarseningMa(model=Magni())\n",
    "\n",
    "# Define the path to the dataset and the target platform (here, \"Tahiti\" refer to a GPU architecture or hardware)\n",
    "dataset_path = \"../../benchmark/Thread\"\n",
    "platform = \"Tahiti\"\n",
    "\n",
    "# Perform data partitioning for training, validation, and testing.\n",
    "# The method 'data_partitioning' returns the split data, including features (X) and labels (y)\n",
    "# Additionally, it returns calibration data and indices for each partition (train, validation, and test).\n",
    "# Args could contain hyperparameters or settings used during data partitioning (such as shuffle, split ratios, etc.)\n",
    "X_cc, y_cc,train_x, valid_x, test_x, train_y, valid_y, test_y, \\\n",
    "        calibration_data,cal_y,train_index, valid_index, test_index,y,X_seq,y_1hot=\\\n",
    "            prom_thread.data_partitioning(dataset_path,platform=platform,mode='train', calibration_ratio=0.1,args=args)\n",
    "\n",
    "# 'X_cc', 'y_cc' : The coarsened features and labels (thread coarsening context)\n",
    "# 'train_x', 'valid_x', 'test_x' : Training, validation, and testing features (input data)\n",
    "# 'train_y', 'valid_y', 'test_y' : Training, validation, and testing labels (output data)\n",
    "# 'calibration_data', 'cal_y' : Calibration data and labels, for model tuning or confidence calibration\n",
    "# 'train_index', 'valid_index', 'test_index' : Indexes for the split datasets (training, validation, test)\n",
    "# 'y', 'X_seq', 'y_1hot' : Other data representations (e.g., one-hot encoding, sequence features/labels)\n",
    "print(\"Data splitting process completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the underlying model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training process for the underlying model...\n",
      "The speedup on the Titan is 93.14%\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting the training process for the underlying model...\")\n",
    "\n",
    "# Initialize the model with the provided arguments\n",
    "# The 'args' contain configurations or hyperparameters needed to set up the model (e.g., learning rate, batch size, etc.)\n",
    "prom_thread.model.init(args)\n",
    "\n",
    "# Train the model using the training data (features and labels)\n",
    "# 'cascading_features' refers to the input features for training (possibly related to thread coarsening features)\n",
    "# 'cascading_y' is the corresponding target labels for the training data\n",
    "# 'verbose=True' enables the printing of training progress or detailed output during the training process\n",
    "prom_thread.model.train(\n",
    "    cascading_features=train_x,\n",
    "    verbose=True,\n",
    "    cascading_y=train_y)\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "# 'origin_speedup_all' will hold the original speedup values\n",
    "# 'speed_up_all' will store the predicted speedup values from the model\n",
    "# 'improved_spp_all' will track any improvements in speedup across predictions\n",
    "origin_speedup_all = []\n",
    "speed_up_all = []\n",
    "improved_spp_all = []\n",
    "\n",
    "# Call 'make_predictionMa' to make predictions using the trained model\n",
    "# Arguments:\n",
    "#   - 'speed_up_all': A list to collect the speedup predictions from the model\n",
    "#   - 'platform': The target platform (e.g., \"Tahiti\") for which the prediction is made\n",
    "#   - 'model': The trained model (prom_thread.model)\n",
    "#   - 'test_x': Features for the test data\n",
    "#   - 'test_index': Indices of the test data\n",
    "#   - 'X_cc': Additional features (possibly coarsened or cascading features)\n",
    "origin_speedup, all_pre, data_distri = make_predictionMa(speed_up_all=speed_up_all,\n",
    "                                                         platform=platform,\n",
    "                                                         model=prom_thread.model,\n",
    "                                                         test_x=test_x,\n",
    "                                                         test_index=test_index,\n",
    "                                                         X_cc=X_cc)\n",
    "\n",
    "# Print the original speedup obtained from the prediction for the specified platform\n",
    "print(f\"The speedup on the Titan is {origin_speedup:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the anomaly detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the construction of the anomaly detector...\n",
      "The anomaly detector has been successfully constructed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting the construction of the anomaly detector...\")\n",
    "\n",
    "# Set the trained classifier model from prom_thread\n",
    "clf = prom_thread.model\n",
    "\n",
    "# Define the prom parameters (method_params) for different evaluation metrics:\n",
    "method_params = {\n",
    "    \"lac\": (\"score\", True),\n",
    "    \"top_k\": (\"top_k\", True),\n",
    "    \"aps\": (\"cumulated_score\", True),\n",
    "    \"raps\": (\"raps\", True)\n",
    "}\n",
    "\n",
    "# Initialize a Prom object for performing conformal prediction and evaluation\n",
    "# 'task' is set to \"thread\" indicating the specific task being modeled\n",
    "Prom_thread = Prom_utils(clf, method_params, task=\"thread\")\n",
    "\n",
    "# Perform conformal prediction:\n",
    "#   - 'cal_x' and 'cal_y' are the calibration data and labels,\n",
    "#   - 'test_x' and 'test_y' are the test data and labels,\n",
    "#   - 'significance_level' is set to \"auto\" for automatic adjustment.\n",
    "y_preds, y_pss, p_value = Prom_thread.conformal_prediction(\n",
    "    cal_x=calibration_data, \n",
    "    cal_y=cal_y, \n",
    "    test_x=test_x,\n",
    "    test_y=test_y,\n",
    "    significance_level=\"auto\"\n",
    ")\n",
    "print(\"The anomaly detector has been successfully constructed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the prom anolamy detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Prom on deployment\n",
    "\n",
    "First, to introduce data drift, we train the ML models on OpenCL benchmarks from two suites and then test the trained model on another left-out benchmark suite.\n",
    "\n",
    "#### Native deployment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading successful, the deployment speedup on the Tahiti is 57.19%\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model for predictions\n",
    "prom_thread.model.restore(r'../../examples/case_study/Thread/ae_savedmodels/tutorial/Tahiti-underlying.model')\n",
    "\n",
    "# Make predictions on the given data\n",
    "origin_speedup, all_pre, data_distri = make_predictionMa(\n",
    "    speed_up_all=speed_up_all,     # Array or list of speed-up values for the model\n",
    "    platform=platform,             # Target platform for prediction (e.g., CPU, GPU)\n",
    "    model=prom_thread.model,       # Loaded model used for making predictions\n",
    "    test_x=test_x,                 # Test features or data input for predictions\n",
    "    test_index=test_index,         # Indices for test data to match predicted values\n",
    "    X_cc=X_cc                      # Contextual data or additional features for prediction\n",
    ")\n",
    "\n",
    "# Print a success message with the deployment speedup result\n",
    "print(f\"Loading successful, the deployment speedup on the {platform} is {origin_speedup:.2%}\")\n",
    "\n",
    "# Calculate the average original speedup from all speed-up values\n",
    "origin = sum(speed_up_all) / len(speed_up_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting drifting samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________The detection performance can be seen below_____________\n",
      "The accuracy for detection on lac is: 100.00%, precision is: 100.00%, recall is: 100.00%, F1 is: 100.00%\n",
      "The accuracy for detection on top_k is: 100.00%, precision is: 100.00%, recall is: 100.00%, F1 is: 100.00%\n",
      "The accuracy for detection on aps is: 100.00%, precision is: 100.00%, recall is: 100.00%, F1 is: 100.00%\n",
      "The accuracy for detection on raps is: 100.00%, precision is: 100.00%, recall is: 100.00%, F1 is: 100.00%\n",
      "The accuracy for detection on mixture is: 100.00%, precision is: 100.00%, recall is: 100.00%, F1 is: 100.00%\n",
      "lac is the best approach，the accuracy is: 100.00%, the precision is: 100.00%, the recall is: 100.00%, the F1 is: 100.00%\n",
      "______________________________________\n"
     ]
    }
   ],
   "source": [
    "# Perform evaluation of the conformal prediction model, retrieving accuracy and other metrics:\n",
    "#   - 'index_all_right' stores all indices where predictions are correct,\n",
    "#   - 'index_list_right' stores lists of indices for each correctly predicted instance,\n",
    "#   - 'Acc_all' holds the overall accuracy of predictions,\n",
    "#   - 'F1_all' stores the F1-score for the predictions,\n",
    "#   - 'Pre_all' holds the precision metric for predictions,\n",
    "#   - 'Rec_all' contains the recall metric for the predictions.\n",
    "index_all_right, index_list_right, Acc_all, F1_all, Pre_all, Rec_all, _, _ = \\\n",
    "    Prom_thread.evaluate_conformal_prediction(\n",
    "        y_preds=y_preds,             # Predicted labels or outcomes\n",
    "        y_pss=y_pss,                 # Prediction scores or probabilities associated with predictions\n",
    "        p_value=p_value,             # p-value threshold for conformal prediction\n",
    "        all_pre=all_pre,             # Array of all prediction values\n",
    "        y=y[test_index],             # True labels or outcomes for test data\n",
    "        significance_level=0.05      # Significance level for conformal prediction intervals\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Improve Deployment Time Performance\n",
    "Prom can enhance the performance of deployed ML systems through incremental learning.\n",
    "\n",
    "#### Retraining the model with incremental learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the most valuable instances for incremental learning...\n",
      "Retraining the model on Tahiti...\n",
      "The retrained speed up is  0.8627036071059495 the improved speed up is  0.2907569525353487\n",
      "Detection Performance Metrics:\n",
      "  - Average Accuracy  : 100.00%\n",
      "  - Average Precision : 100.00%\n",
      "  - Average Recall    : 100.00%\n",
      "  - Average F1 Score  : 100.00%\n",
      "Final improved speed-up percentage: 29.08%\n"
     ]
    }
   ],
   "source": [
    "# Perform incremental learning by selecting the most valuable instances to add to the training set:\n",
    "#   - 'seed_value' is used for randomization,\n",
    "#   - 'test_index' and 'train_index' are updated accordingly.\n",
    "print(\"Finding the most valuable instances for incremental learning...\")\n",
    "train_index, test_index = Prom_thread.incremental_learning(\n",
    "    seed_value, \n",
    "    test_index, \n",
    "    train_index\n",
    ")\n",
    "# Fine-tune the model using the updated training set:\n",
    "#   - 'X_seq[train_index]' is the updated training data,\n",
    "#   - 'y_1hot[train_index]' are the corresponding labels in one-hot encoding format.\n",
    "print(f\"Retraining the model on {platform}...\")\n",
    "prom_thread.model.fine_tune(\n",
    "    cascading_features=X_seq[train_index],\n",
    "    cascading_y=y_1hot[train_index],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test the fine-tuned model and make predictions using the updated model:\n",
    "#   - 'speed_up_all' stores the predicted speedup values,\n",
    "#   - 'improved_spp_all' stores improvements in speedup,\n",
    "#   - 'origin_speedup' refers to the initial speedup before fine-tuning.\n",
    "retrained_speedup, inproved_speedup, data_distri = make_prediction_ilMa(\n",
    "    speed_up_all=speed_up_all, \n",
    "    platform=platform,\n",
    "    model=prom_thread.model,\n",
    "    test_x=X_seq[test_index],\n",
    "    test_index=test_index, \n",
    "    X_cc=X_cc,\n",
    "    origin_speedup=origin_speedup,\n",
    "    improved_spp_all=improved_spp_all\n",
    ")\n",
    "\n",
    "origin_speedup_all.append(origin_speedup)\n",
    "speed_up_all.append(retrained_speedup)\n",
    "improved_spp_all.append(inproved_speedup)\n",
    "\n",
    "mean_acc = sum(Acc_all) / len(Acc_all)\n",
    "mean_f1 = sum(F1_all) / len(F1_all)\n",
    "mean_pre = sum(Pre_all) / len(Pre_all)\n",
    "mean_rec = sum(Rec_all) / len(Rec_all)\n",
    "\n",
    "# Calculate the improved mean speed-up (improved_spp_all is a list of improved speed-ups)\n",
    "meanimproved_speed_up = sum(improved_spp_all) / len(improved_spp_all)\n",
    "\n",
    "# Output the model's average accuracy, precision, recall, and F1 score, formatted as percentages\n",
    "print(\n",
    "    \"Detection Performance Metrics:\\n\"\n",
    "    f\"  - Average Accuracy  : {mean_acc * 100:.2f}%\\n\"\n",
    "    f\"  - Average Precision : {mean_pre * 100:.2f}%\\n\"\n",
    "    f\"  - Average Recall    : {mean_rec * 100:.2f}%\\n\"\n",
    "    f\"  - Average F1 Score  : {mean_f1 * 100:.2f}%\"\n",
    ")\n",
    "\n",
    "\n",
    "# Output the final improved speed-up as a percentage\n",
    "print(f\"Final improved speed-up percentage: {meanimproved_speed_up * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 2: Experimental Evaluation\n",
    "\n",
    "Here, we provide a small-sized evaluation to showcase the working mechanism of the Prom on five case studies. A full-scale evaluation, which takes more than a day to run, is provided through the Docker image (with detailed instructions on our project Github).\n",
    "\n",
    "### Case Study 1: Thread Coarsening (Section 6.1)\n",
    "\n",
    "This problem develops a model to determine the optimal OpenCL GPU thread coarsening factor for performance optimization. Following other works, an ML model predicts a coarsening factor (ranging  from 1 to 32) for a test OpenCL kernel, where 1 indicates no coarsening. Underlying models. We train the baseline model using  leave-one-out cross-validation, which involves training the base-  line model on 16 OpenCL kernels and testing on another one. We  then repeat this process until all benchmark suites have been tested  once. To introduce data drift, we train the ML models on OpenCL  benchmarks from two suites and then test the trained model on  another left-out benchmark suite.\n",
    "\n",
    "This demo corresponds to Figure 7(a), 8(a), 9(a), 11(a) of the submitted manuscript.\n",
    "\n",
    "*approximate runtime = 10 minutes for one benchmark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment for 'C1,C2,C4' activated.\n",
      "\n",
      "The evaluation on DeepTune\n",
      "\n",
      "_________Start training phase________\n",
      "Loading dataset...\n",
      "Training underlying model...\n",
      "Training model successful, the speedup is 93.14%\n",
      "Training phase finished. The speedup percentage is: 0.93\n",
      "_________Start deploy phase________\n",
      "Loading dataset...\n",
      "Loading underlying model...\n",
      "Loading successful, the speedup during deployment is 78.43%\n",
      "Start conformal prediction...\n",
      "_____________The detection performance can be seen below_____________\n",
      "The accuracy for detection on lac is: 100.00%, precision is: 100.00%, recall is: 100.00%, F1 is: 100.00%\n",
      "The accuracy for detection on top_k is: 100.00%, precision is: 100.00%, recall is: 100.00%, F1 is: 100.00%\n",
      "The accuracy for detection on aps is: 100.00%, precision is: 100.00%, recall is: 100.00%, F1 is: 100.00%\n",
      "The accuracy for detection on raps is: 100.00%, precision is: 100.00%, recall is: 100.00%, F1 is: 100.00%\n",
      "The accuracy for detection on mixture is: 100.00%, precision is: 100.00%, recall is: 100.00%, F1 is: 100.00%\n",
      "lac is the best approach，the accuracy is: 100.00%, the precision is: 100.00%, the recall is: 100.00%, the F1 is: 100.00%\n",
      "______________________________________\n",
      "Finding the most valuable instances for incremental learning...\n",
      "Retraining the model...\n",
      "The retrained speed up is  1.0 the improved speed up is  0.2157339275208775\n",
      "The average accuracy is: 100.00%, average precision is: 100.00%, average recall is: 100.00%, average F1 is: 100.00%, \n",
      "The speedup of retrained model is: 0.2157339275208775\n",
      "\n",
      "The evaluation on Magni\n",
      "\n",
      "_________Start training phase________\n",
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "!python ae_thread.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Full-scale evaluation data\n",
    "\n",
    "We now plot the diagrams using full-scale evaluation data. \n",
    "The results correspond to Figure 7(a), 8(a), 9(a) and 11(a) of the submitted manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%run ae_plot.py --thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 2: Loop Vectorization (Section 6.2)\n",
    "\n",
    "This task constructs a predictive model to determine the optimal Vectorization Factor (VF) and Interleaving Factor (IF) for individual  vectorizable loops in C programs [34, 48]. Following [34], we ex-  plore 35 combinations of VF (1, 2, 4, 8, 16, 32, 64) and IF (1, 2, 4, 8, 16). We initially allocate 80% (4800)  of loop programs for training the model, reserving the remaining 20% (1200) for testing its performance. To introduce data drift, we  use loop programs generated from 14 benchmarks for training  and evaluate the model on the programs from the remaining 4  benchmarks. This ensures that the function and content of test  samples are not encountered during the training phase.\n",
    "\n",
    "This demo corresponds to Figure 7(b), 8(b), 9(b) and 11(b) of the submitted manuscript.\n",
    "\n",
    "*approximate runtime = 10 minutes for one benchmark*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python ae_loop.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Full-scale evaluation data\n",
    "\n",
    "We now plot the diagrams using full-scale evaluation data. \n",
    "The results correspond to Figure 7(b), 8(b), 9(b) and 11(b) of the submitted manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%run ae_plot.py --loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 3: Heterogeneous Mapping (Section 6.3)\n",
    "\n",
    "This task develops a binary classifier to determine if the CPU or  the GPU gives faster performance for an OpenCL kernel. We train and evaluate the baseline  model using 10-fold cross-validation. This involves training a model  on programs from all but one of the sets and then testing it on the  programs from the remaining set. To introduce data drift, we train  the models using 6 benchmark suites and then test the trained  models on the remaining suite. We repeat this process until all  benchmark suites have been tested at least once.\n",
    "\n",
    "This demo corresponds to Figure 7(c), 8(c), 9(c) and 11(c) of the submitted manuscript.\n",
    "\n",
    "*approximate runtime = 10 minutes for one benchmark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Traceback (most recent call last):\n",
      "  File \"ae_dev.py\", line 5, in <module>\n",
      "    from ae_DevM_Deeptune import ae_dev_deep\n",
      "  File \"../case_study/DeviceM/ae_DevM_Deeptune.py\", line 12, in <module>\n",
      "    from examples.case_study.DeviceM.compy.models.graphs.pytorch_geom_model import Dev_gnn\n",
      "  File \"/home/huanting/PROM/examples/case_study/DeviceM/compy/__init__.py\", line 1, in <module>\n",
      "    import compy.datasets\n",
      "  File \"../case_study/DeviceM/compy/__init__.py\", line 1, in <module>\n",
      "    import compy.datasets\n",
      "  File \"../case_study/DeviceM/compy/datasets/__init__.py\", line 1, in <module>\n",
      "    from .dataset import Dataset\n",
      "  File \"../case_study/DeviceM/compy/datasets/dataset.py\", line 6, in <module>\n",
      "    from appdirs import user_data_dir\n",
      "ModuleNotFoundError: No module named 'appdirs'\n"
     ]
    }
   ],
   "source": [
    "!python ae_dev.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Full-scale evaluation data\n",
    "\n",
    "We now plot the diagrams using full-scale evaluation data. \n",
    "The results correspond to Figure 7(c), 8(c), 9(c) and 11(c) of the submitted manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%run ae_plot.py --dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 4: Vulnerability Detection (Section 6.4)\n",
    "\n",
    "This task develops an ML classifier to predict if a given C function  contains a potential code vulnerability.\n",
    "\n",
    "This demo corresponds to Figure 7(d), 8(d), 9(d) and 11(d) of the submitted manuscript. We consider the top-8 most dangerous types of bugs from the 2023 CWE. As with prior approaches, we initially train the  model on 80% of the randomly selected samples and evaluate its  performance on the remaining 20% samples. Then, we introduce  data drift by training the model on data collected between 2013 and 2020 and testing the trained model on samples collected between 2021 and 2023.\n",
    "\n",
    "*approximate runtime = 10 minutes for one benchmark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python ae_vul.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Full-scale evaluation data\n",
    "\n",
    "We now plot the diagrams using full-scale evaluation data. \n",
    "The results correspond to Figure 7(d), 8(d), 9(d) and 11(d) of the submitted manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%run ae_plot.py --vul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 5: DNN Code Generation (Section 6.5)\n",
    "\n",
    "This task builds a regression-based cost model to drive the schedule  search process in TVM for DNN code generation on multi-core CPUs. The cost model estimates the potential gain of a schedule (e.g., instruction orders and data placement) to guide the search. For the baseline, we train and test the cost model on  the BERT-base dataset, where the model is trained on 80% randomly  selected samples and then tested on the remaining 20% samples. To introduce data drift, we tested the trained model on the other  three variants of the BERT model and ResNet-50.\n",
    "\n",
    "This demo corresponds to Table 2 of the submitted manuscript.\n",
    "\n",
    "*approximate runtime = 10 minutes for one benchmark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python ae_tlp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Full-scale evaluation data\n",
    "\n",
    "We now plot the diagrams using full-scale evaluation data. \n",
    "The results correspond to Table 3 and Figure 8(e) of the submitted manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%run ae_plot.py --tlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to Other CP-based Methods (Section 7.5)\n",
    "\n",
    "This experiment compares Prom with conformal prediction-based methods like MAPIE and PUNCC, as well as RISE, developed for wireless sensing, and TESSERACT, designed for malware classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The results correspond to Figure 10.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}