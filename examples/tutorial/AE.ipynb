{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Enhancing Deployment-time Predictive Model Robustness for Code Analysis and Optimization: Artifacts Evaluation Instructions\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "This interactive Jupyter notebook provides a small-scale demo showcasing the evaluation of case studies reported in the paper.\n",
    "\n",
    "The main results of our CGO 2025 paper demonstrate the application of Prom across five case studies for detecting drifting samples. The full evaluation, presented in our paper, was conducted on a much larger dataset over a longer runtime. This notebook contains minimal working examples, designed to be evaluated within a reasonable amount of time (approximately 2 hours).\n",
    "\n",
    "## Instructions for Experimental Workflow:\n",
    "\n",
    "Before starting. Go to the landing page, select the checkbox next to the notebook titled *main.ipynb*, and then click \"**Duplicate**\".\n",
    "\n",
    "Click the name of the newly created Jupyter Notebook, e.g. **AE-Copy1.ipynb**. Next, select \"**Kernel**\" > \"**Restart & Clear Output**\". Next, repeatedly press the play button (the tooltip is \"run cell, select below\") to step through each cell of the notebook.\n",
    "\n",
    "Alternatively, you can select each cell in turn and use \"**Cell**\"> \"**Run Cell**\" from the menu to run specific cells. Note that some cells depend on previous cells being executed. If any errors occur, ensure that all previous cells have been run.\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "**Some cells can take more than 30 minutes to complete; please wait for the results until step to the next cell.**\n",
    "\n",
    "High load can lead to a longer wait times for results, particularly if multiple reviewers are generating results simulataneously.\n",
    "\n",
    "The experiments are customisable, as the code withinin the Jupyter Notebook can be editeddirectly. Simply type your changes into the code blocks and re-run them using **Cell > Run Cells** from the menu.\n",
    "\n",
    "## Links to The Paper\n",
    "\n",
    "For each step, we note the section number of the submitted version where the relevant technique is described or data is presented.\n",
    "\n",
    "The main results are presented in Figures 7-10 and Table 2 and 3 of the submitted paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 1: Tutorial for Prom\n",
    "\n",
    "This demo corresponds to the simplified drifting detection example shown in Figure 2. Note that the code has been refactored, resulting in minor changes to the API. This small-scale demo represents case study 1 on thread coarsening. \n",
    "\n",
    "## Step 1. Train the underlying model\n",
    "\n",
    "This problem involves developing a model to determine the optimal OpenCL GPU thread coarsening factor for performance optimization. Following the original paper, an ML model predicts a coarsening factor (ranging from 1 to 32) for a test OpenCL kernel, where 1 indicates no coarsening. Following the setup of DeepTune, we train and test the models on their labeled dataset, which includes 17 OpenCL kernels from three benchmark suites across four GPU platforms (this minimal working example runs on the Titan platform).\n",
    "\n",
    "We train the baseline model using leave-one-out cross-validation, where the model is trained on 16 OpenCL kernels and tested on the remaining one.\n",
    "#### Training dataset and calibration dataset partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment activated.\r\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ConformityScore' from 'prom.conformity_scores' (/cgo/prom/PROM/src/prom/conformity_scores/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_778/407582267.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cgo/prom/PROM/thirdpackage'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mMagni_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mThreadCoarseningMa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMagni\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_predictionMa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_prediction_ilMa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mThread_magni\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_magni_args_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_magni_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprom_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProm_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cgo/prom/PROM/examples/case_study/Thread/Magni_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprom_classification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMapieClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m from prom.metrics import (classification_coverage_score,\n\u001b[1;32m     25\u001b[0m                            classification_mean_width_score)\n",
      "\u001b[0;32m/cgo/prom/PROM/src/prom/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m __all__ = [\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"regression\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cgo/prom/PROM/src/prom/classification.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_machine_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrayLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_mean_width_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m from .utils import (check_alpha, check_alpha_and_n_samples, check_cv,\n\u001b[1;32m     21\u001b[0m                     \u001b[0mcheck_estimator_classification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_n_features_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cgo/prom/PROM/src/prom/metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_machine_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrayLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from .utils import (calc_bins, check_alpha, check_array_inf, check_array_nan,\n\u001b[0m\u001b[1;32m     11\u001b[0m                     \u001b[0mcheck_array_shape_classification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mcheck_array_shape_regression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_arrays_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cgo/prom/PROM/src/prom/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_compatibility\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_quantile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrayLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconformity_scores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAbsoluteConformityScore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConformityScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mSPLIT_STRATEGIES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"uniform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quantile\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array split\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cgo/prom/PROM/src/prom/conformity_scores/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconformity_scores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConformityScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from .residual_conformity_scores import (AbsoluteConformityScore,\n\u001b[1;32m      3\u001b[0m                                          \u001b[0mGammaConformityScore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                          ResidualNormalisedScore)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cgo/prom/PROM/src/prom/conformity_scores/conformity_scores.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compatibility\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_nanquantile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrayLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnsembleEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cgo/prom/PROM/src/prom/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m __all__ = [\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"regression\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cgo/prom/PROM/src/prom/regression/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquantile_regression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMapieQuantileRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMapieRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtime_series_regression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMapieTimeSeriesRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m __all__ = [\n",
      "\u001b[0;32m/cgo/prom/PROM/src/prom/regression/time_series_regression.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrayLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconformity_scores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConformityScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMapieRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_gamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ConformityScore' from 'prom.conformity_scores' (/cgo/prom/PROM/src/prom/conformity_scores/__init__.py)"
     ]
    }
   ],
   "source": [
    "!bash ae_tu.sh\n",
    "# import sys\n",
    "# print(sys.version)\n",
    "# print(sys.executable)\n",
    "# !conda info --env\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "sys.path.append('/cgo/prom/PROM')\n",
    "sys.path.append('../case_study/Thread/')\n",
    "sys.path.append('/cgo/prom/PROM/src')\n",
    "sys.path.append('/cgo/prom/PROM/thirdpackage')\n",
    "\n",
    "from Magni_utils import ThreadCoarseningMa, Magni, make_predictionMa, make_prediction_ilMa\n",
    "from Thread_magni import load_magni_args_notebook, load_magni_args\n",
    "from src.prom.prom_util import Prom_utils\n",
    "\n",
    "print(\"Starting to partition the training and calibration datasets...\")\n",
    "\n",
    "# Load necessary arguments for the program, related to model configuration or runtime settings\n",
    "args = load_magni_args()\n",
    "seed_value = int(args.seed)\n",
    "\n",
    "# Initialize a thread coarsening model using the approach in Magni et al\n",
    "prom_thread = ThreadCoarseningMa(model=Magni())\n",
    "\n",
    "# Define the path to the dataset and the target platform (here, \"Tahiti\" refers to a GPU architecture or hardware)\n",
    "dataset_path = \"../../benchmark/Thread\"\n",
    "platform = \"Tahiti\"\n",
    "\n",
    "# Perform data partitioning for training, validation, and testing\n",
    "# returns the split data, including features (X) and labels (y)\n",
    "# returns calibration data and indices for each partition (train, validation, and test).\n",
    "# args can contain data partitioning hyperparameters (such as shuffle, split ratios, etc.)\n",
    "\n",
    "split_data = prom_thread.data_partitioning(\n",
    "    dataset_path, platform=platform, mode='train', calibration_ratio=0.1, args=args\n",
    ")\n",
    "(\n",
    "    X_cc, y_cc,               # The coarsened features and labels (thread coarsening context)\n",
    "    train_x, valid_x, test_x, # Training, validation, and testing features (input data)\n",
    "    train_y, valid_y, test_y, # Training, validation, and testing labels (output data)\n",
    "    calib_data, calib_y,      # Calibration data and labels, for model tuning or confidence calibration\n",
    "    train_index, valid_index, test_index, # Indexes for the split datasets (training, validation, test)\n",
    "    y, X_seq, y_1hot          # Other data representations (e.g., one-hot encoding, sequence features/labels)\n",
    ") = split_data\n",
    "\n",
    "print(\"Data splitting process completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the underlying model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Starting the training process for the underlying model...\")\n",
    "\n",
    "# Initialize the model\n",
    "# 'args' can contain model hyperparameters (e.g., learning rate, batch size, etc.)\n",
    "prom_thread.model.init(args)\n",
    "\n",
    "# Train the model using training data (features and labels)\n",
    "prom_thread.model.train(\n",
    "    cascading_features=train_x, # train data features (possibly related to thread coarsening features)\n",
    "    verbose=True,               # detailed training process output including progress\n",
    "    cascading_y=train_y         # train data labels\n",
    ")\n",
    "\n",
    "origin_speedup_all = [] # original speedup values\n",
    "speed_up_all = []       # predicted speedup values\n",
    "improved_spp_all = []   # speedup improvement values (original speedup - retrained speedup)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "origin_speedup, all_pre, data_distri = make_predictionMa(\n",
    "    speed_up_all=speed_up_all,    # speedup predictions from the model (list)\n",
    "    platform=platform,            # target platform (e.g., \"Tahiti\") used for prediction\n",
    "    model=prom_thread.model,      # trained model\n",
    "    test_x=test_x,                # test data features\n",
    "    test_index=test_index,        # test data indices\n",
    "    X_cc=X_cc                     # additional features (possibly coarsened or cascading features)\n",
    ")\n",
    "\n",
    "print(f\"Thread coarsening speedup on platform '{platform}' is {origin_speedup:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the anomaly detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Starting the construction of the anomaly detector...\")\n",
    "\n",
    "# trained classifier model\n",
    "clf = prom_thread.model\n",
    "\n",
    "# prom parameters (method_params) for different evaluation metrics\n",
    "method_params = {\n",
    "    \"lac\": (\"score\", True),\n",
    "    \"top_k\": (\"top_k\", True),\n",
    "    \"aps\": (\"cumulated_score\", True),\n",
    "    \"raps\": (\"raps\", True)\n",
    "}\n",
    "\n",
    "# Prom object for performing conformal prediction and evaluation\n",
    "# 'task' : set to \"thread\" to model thread coarsening task\n",
    "Prom_thread = Prom_utils(clf, method_params, task=\"thread\")\n",
    "\n",
    "# Perform conformal prediction\n",
    "y_preds, y_pss, p_value = Prom_thread.conformal_prediction(\n",
    "    cal_x=calib_data, cal_y=calib_y, # calibration data and labels\n",
    "    test_x=test_x, test_y=test_y,    # test data and labels\n",
    "    significance_level=\"auto\"        # set to \"auto\" for automatic adjustment\n",
    ")\n",
    "print(\"The anomaly detector has been successfully constructed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Prom anolamy detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Prom on deployment\n",
    "\n",
    "First, to introduce data drift, we train the ML models on OpenCL benchmarks from two suites and then test the trained model on another left-out benchmark suite.\n",
    "\n",
    "#### Native deployment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model for predictions\n",
    "prom_thread.model.restore(r'../../examples/case_study/Thread/ae_savedmodels/tutorial/Tahiti-underlying.model')\n",
    "\n",
    "# Make predictions on test data\n",
    "origin_speedup, all_pre, data_distri = make_predictionMa(\n",
    "    speed_up_all=speed_up_all,    # Array or list of speed-up values for the model\n",
    "    platform=platform,            # Target platform for prediction (e.g., CPU, GPU)\n",
    "    model=prom_thread.model,      # Loaded model used for making predictions\n",
    "    test_x=test_x,                # Test features or data input for predictions\n",
    "    test_index=test_index,        # Indices for test data to match predicted values\n",
    "    X_cc=X_cc                     # Contextual data or additional features for prediction\n",
    ")\n",
    "\n",
    "# Print a success message with the deployment speedup result\n",
    "print(f\"Deployment speedup on platform '{platform}' is {origin_speedup:.2%}\")\n",
    "\n",
    "# Calculate average from all speed-up values\n",
    "origin = sum(speed_up_all) / len(speed_up_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting drifting samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# retrieves evaluation metrics for conformal prediction model\n",
    "(\n",
    "    index_ncm_correct, # each row contains indices flagged correctly by that ncm function\n",
    "    index_all_correct, # each element contains indices flagged correctly by all ncm functions\n",
    "    Acc_all, F1_all, Pre_all, Rec_all, # accuracy, f1 score, precision and recall\n",
    "    *_\n",
    ") = Prom_thread.evaluate_conformal_prediction(\n",
    "    y_preds=y_preds,             # Predicted labels\n",
    "    y_pss=y_pss,                 # Prediction scores or probabilities\n",
    "    p_value=p_value,             # p-value threshold for conformal prediction\n",
    "    all_pre=all_pre,             # all prediction values\n",
    "    y=y[test_index],             # True labels or outcomes for test data\n",
    "    significance_level=0.05      # Significance level for conformal prediction intervals\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Improve Deployment Time Performance\n",
    "Prom can enhance the performance of deployed ML systems through incremental learning.\n",
    "\n",
    "#### Retraining the model with incremental learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most valuable test instances are moved to the training set\n",
    "print(\"Finding the most valuable instances for incremental learning...\")\n",
    "train_index, test_index = Prom_thread.incremental_learning(\n",
    "    seed_value,\n",
    "    test_index,\n",
    "    train_index\n",
    ")\n",
    "# Fine-tune the model with updated training set\n",
    "print(f\"Retraining the model on platform '{platform}'...\")\n",
    "prom_thread.model.fine_tune(\n",
    "    cascading_features=X_seq[train_index], # updated training features\n",
    "    cascading_y=y_1hot[train_index], # updated one-hot encoded training labels\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Make predictions using the updated model\n",
    "retrained_speedup, improved_speedup, y_preds = make_prediction_ilMa(\n",
    "    speed_up_all=speed_up_all,        # predicted speedup values\n",
    "    platform=platform,\n",
    "    model=prom_thread.model,\n",
    "    test_x=X_seq[test_index],\n",
    "    test_index=test_index,\n",
    "    X_cc=X_cc,\n",
    "    origin_speedup=origin_speedup,    # speedup before fine-tuning\n",
    "    improved_spp_all=improved_spp_all # speedup improvements\n",
    ")\n",
    "\n",
    "origin_speedup_all.append(origin_speedup)\n",
    "speed_up_all.append(retrained_speedup)\n",
    "improved_spp_all.append(improved_speedup)\n",
    "\n",
    "mean_acc = sum(Acc_all) / len(Acc_all)\n",
    "mean_f1 = sum(F1_all) / len(F1_all)\n",
    "mean_pre = sum(Pre_all) / len(Pre_all)\n",
    "mean_rec = sum(Rec_all) / len(Rec_all)\n",
    "\n",
    "# Calculate the average speedup improvement\n",
    "mean_improved_speed_up = sum(improved_spp_all) / len(improved_spp_all)\n",
    "\n",
    "# Output the model's average accuracy, precision, recall, and F1 score, formatted as percentages\n",
    "print(\n",
    "    \"Detection Performance Metrics:\\n\"\n",
    "    f\"  - Average Accuracy  : {mean_acc * 100:.2f}%\\n\"\n",
    "    f\"  - Average Precision : {mean_pre * 100:.2f}%\\n\"\n",
    "    f\"  - Average Recall    : {mean_rec * 100:.2f}%\\n\"\n",
    "    f\"  - Average F1 Score  : {mean_f1 * 100:.2f}%\"\n",
    ")\n",
    "\n",
    "\n",
    "# Output the final improved speedup as a percentage\n",
    "print(f\"Final improved speedup percentage: {mean_improved_speed_up * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 2: Experimental Evaluation\n",
    "\n",
    "Here, we provide a small-sized evaluation to showcase the working mechanism of the Prom on five case studies. A full-scale evaluation, which takes more than a day to run, is provided through the Docker image.\n",
    "\n",
    "### Case Study 1: Thread Coarsening (Section 6.1)\n",
    "\n",
    "This problem develops a model to determine the optimal OpenCL GPU thread coarsening factor for performance optimization. Following other works, an ML model predicts a coarsening factor (ranging  from 1 to 32) for a test OpenCL kernel, where 1 indicates no coarsening. Underlying models. We train the baseline model using  leave-one-out cross-validation, which involves training the baseline model on 16 OpenCL kernels and testing on another one. We  then repeat this process until all benchmark suites have been tested  once. To introduce data drift, we train the ML models on OpenCL  benchmarks from two suites and then test the trained model on  another left-out benchmark suite.\n",
    "\n",
    "This demo corresponds to Figure 7(a), 8(a), 9(a), 11(a) of the submitted manuscript.\n",
    "\n",
    "*approximate runtime = 10 minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python ae_thread.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Full-scale evaluation data\n",
    "\n",
    "We now plot the diagrams using full-scale evaluation data. \n",
    "The results correspond to Figure 7(a), 8(a), 9(a) and 11(a) of the submitted manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%run ae_plot.py --case thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 2: Loop Vectorization (Section 6.2)\n",
    "\n",
    "This task constructs a predictive model to determine the optimal Vectorization Factor (VF) and Interleaving Factor (IF) for individual  vectorizable loops in C programs [34, 48]. Following [34], we ex-  plore 35 combinations of VF (1, 2, 4, 8, 16, 32, 64) and IF (1, 2, 4, 8, 16). We initially allocate 80% of loop programs for training the model, reserving the remaining 20% for testing its performance. To introduce data drift, we  use loop programs generated from 14 benchmarks for training  and evaluate the model on the programs from the remaining 4 benchmarks. This ensures that the function and content of test  samples are not encountered during the training phase.\n",
    "\n",
    "This demo corresponds to Figure 7(b), 8(b), 9(b) and 11(b) of the submitted manuscript.\n",
    "\n",
    "*approximate runtime = 10 minutes*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python ae_loop.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Full-scale evaluation data\n",
    "\n",
    "We now plot the diagrams using full-scale evaluation data. \n",
    "The results correspond to Figure 7(b), 8(b), 9(b) and 11(b) of the submitted manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%run ae_plot.py --case loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 3: Heterogeneous Mapping (Section 6.3)\n",
    "\n",
    "This task develops a binary classifier to determine whether the CPU or  the GPU gives faster performance for an OpenCL kernel. We train and evaluate the baseline  model using 10-fold cross-validation. This involves training a model on programs from all but one of the sets and then testing it on the programs from the remaining set. To introduce data drift, we train the models using 6 benchmark suites and then test the trained  models on the remaining suite. We repeat this process until all  benchmark suites have been tested at least once.\n",
    "\n",
    "This demo corresponds to Figure 7(c), 8(c), 9(c) and 11(c) of the submitted manuscript.\n",
    "\n",
    "*approximate runtime = 20 minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "target_directory = os.path.abspath(\"../case_study/DeviceM/\")\n",
    "target_file = \"ae_dev.sh\"\n",
    "if not os.path.isfile(target_file):\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Switched to directory: {target_directory}\")\n",
    "\n",
    "\n",
    "!bash ae_dev.sh\n",
    "\n",
    "if os.path.isfile(target_file):\n",
    "    os.chdir(\"../../tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Full-scale evaluation data\n",
    "\n",
    "We now plot the diagrams using full-scale evaluation data. \n",
    "The results correspond to Figure 7(c), 8(c), 9(c) and 11(c) of the submitted manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%run ae_plot.py --case dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 4: Vulnerability Detection (Section 6.4)\n",
    "\n",
    "This task develops an ML classifier to predict if a given C function  contains a potential code vulnerability.\n",
    "\n",
    "This demo corresponds to Figure 7(d), 8(d), 9(d) and 11(d) of the submitted manuscript. We consider the top-8 most dangerous types of bugs from the 2023 CWE. As with prior approaches, we initially train the  model on 80% of the randomly selected samples and evaluate its  performance on the remaining 20% samples. Then, we introduce  data drift by training the model on data collected between 2013 and 2023 and testing the trained model on samples collected between 2021 and 2023.\n",
    "\n",
    "*approximate runtime = 30 minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python ae_vul.py --output_dir=./saved_models     --model_type=roberta     --tokenizer_name=microsoft/codebert-base     --model_name_or_path=microsoft/codebert-base   --do_train  --do_eval     --do_test     --train_data_file=../../benchmark/Bug/train.jsonl     --eval_data_file=../../benchmark/Bug/valid.jsonl     --test_data_file=../../benchmark/Bug/test.jsonl --evaluate_during_training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Full-scale evaluation data\n",
    "\n",
    "We now plot the diagrams using full-scale evaluation data. \n",
    "The results correspond to Figure 7(d), 8(d), 9(d) and 11(d) of the submitted manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%run ae_plot.py --case vul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 5: DNN Code Generation (Section 6.5)\n",
    "\n",
    "This task builds a regression-based cost model to drive the schedule  search process in TVM for DNN code generation on multi-core CPUs. The cost model estimates the potential gain of a schedule (e.g., instruction orders and data placement) to guide the search. For the baseline, we train and test the cost model on  the BERT-base dataset, where the model is trained on 80% randomly  selected samples and then tested on the remaining 20% samples. To introduce data drift, we tested the trained model on the other  three variants of the BERT model and ResNet-50.\n",
    "\n",
    "This demo corresponds to Table 2 of the submitted manuscript.\n",
    "\n",
    "*approximate runtime = 20 minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!bash ae_tlp.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Full-scale evaluation data\n",
    "\n",
    "We now plot the diagrams using full-scale evaluation data. \n",
    "The results correspond to Table 3 and Figure 8(e) of the submitted manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%run ae_plot.py --case tlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to Other CP-based Methods (Section 7.5)\n",
    "\n",
    "This experiment compares Prom with conformal prediction-based methods like MAPIE and PUNCC, as well as RISE, developed for wireless sensing, and TESSERACT, designed for malware classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The results correspond to Figure 10.\n",
    "!bash ae_compare.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Full-scale evaluation data\n",
    "\n",
    "We now plot the diagrams using full-scale evaluation data. \n",
    "The results correspond to Figure 10 of the submitted manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%run ae_plot.py --case compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##  Further Analysis (Optional)\n",
    "\n",
    "This section presents an analysis of certain parameters, corresponding to Section 7.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results correspond to Figure 13(a).\n",
    "%run ae_plot.py --case gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results correspond to Figure 13(b).\n",
    "!bash ae_cd.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ae_plot.py --case cd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thread",
   "language": "python",
   "name": "thread"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
