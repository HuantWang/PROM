<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>tlp_fine_tune &mdash; Prom v0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Prom
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about.html">About</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../src/prom.html">prom package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../src/prom.conformity_scores.html">prom.conformity_scores package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../src/prom.control_risk.html">prom.control_risk package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../src/prom.estimator.html">prom.estimator package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../src/prom.regression.html">prom.regression package</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Thread Coarsening API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../case_study/thread/Deeptune_utils.html">Deeptune_utils module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/thread/Magni_utils.html">Magni_utils module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/thread/Thread_Deep.html">Thread_Deep module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/thread/Thread_magni.html">Thread_magni module</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Loop Tiling API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../case_study/Loop/Deeptune_utils.html">Deeptune_utils module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/Loop/Loop_de.html">Loop_de module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/Loop/Loop_ma.html">Loop_ma module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/Loop/Loop_SVM.html">Loop_SVM module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/Loop/Loop_thread_sensitive.html">Loop_thread_sensitive module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/Loop/Magni_utils.html">Magni_utils module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/Loop/Ml_utils.html">Ml_utils module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/Loop/prom_util_sensitive.html">prom_util_sensitive module</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device Mapping API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../case_study/DeviceM/compy.models.graphs.html">compy.models.graphs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/DeviceM/compy.models.seqs.html">compy.models.seqs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/DeviceM/compy.representations.html">compy.representations package</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Bug Detection API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../case_study/BugD/model.html">model module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/BugD/preprocess.html">preprocess module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/BugD/VD_codebert.html">VD_codebert module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/BugD/VD_vulde.html">VD_vulde module</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tensor Tuning API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../case_study/tlp/bert_large.html">bert_large module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/tlp/bert_med.html">bert_med module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/tlp/bert_tiny.html">bert_tiny module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/tlp/sensitive.html">sensitive module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/tlp/tlp_fine_tune.html">tlp_fine_tune module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../case_study/tlp/train_tlp.html">train_tlp module</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Prom</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Module code</a></li>
      <li class="breadcrumb-item active">tlp_fine_tune</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for tlp_fine_tune</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">argparse</span>


<div class="viewcode-block" id="AttentionModule"><a class="viewcode-back" href="../case_study/tlp/tlp_fine_tune.html#tlp_fine_tune.AttentionModule">[docs]</a><span class="k">class</span> <span class="nc">AttentionModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Defines a neural network module with an attention mechanism for encoding input features, processing with residual blocks, and decoding to output.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    fea_size : int</span>
<span class="sd">        Dimension of input features.</span>
<span class="sd">    step_size : int</span>
<span class="sd">        Number of time steps in input data.</span>
<span class="sd">    res_block_cnt : int</span>
<span class="sd">        Number of residual blocks in the network.</span>

<span class="sd">    encoder : nn.Sequential</span>
<span class="sd">        Encoder section that transforms input features into high-dimensional representations.</span>
<span class="sd">    attention : nn.MultiheadAttention</span>
<span class="sd">        Multi-head attention mechanism to capture dependencies between features.</span>
<span class="sd">    l_list : nn.Sequential</span>
<span class="sd">        Sequence of residual blocks for further feature processing.</span>
<span class="sd">    decoder : nn.Sequential</span>
<span class="sd">        Decoder section that transforms the final representation to the desired output.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fea_size</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">fea_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_size</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">step_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">res_block_cnt</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">res_block_cnt</span>

        <span class="n">in_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fea_size</span>
        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">hidden_dim</span>
        <span class="n">out_dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">out_dim</span>
        <span class="n">hidden_dim_1</span> <span class="o">=</span> <span class="n">hidden_dim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">hidden_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">hidden_dim</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">hidden_dim</span><span class="p">[</span><span class="mi">3</span><span class="p">]),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span>
            <span class="n">hidden_dim_1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">attention_head</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">l0</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim_1</span><span class="p">,</span> <span class="n">hidden_dim_1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">res_block_cnt</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">l_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim_1</span><span class="p">,</span> <span class="n">hidden_dim_1</span><span class="p">),</span> 
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
            <span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l_list</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">l_list</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim_1</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">out_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">out_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">out_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">out_dim</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">out_dim</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">out_dim</span><span class="p">[</span><span class="mi">3</span><span class="p">]),</span>
        <span class="p">)</span>

<div class="viewcode-block" id="AttentionModule.forward"><a class="viewcode-back" href="../case_study/tlp/tlp_fine_tune.html#tlp_fine_tune.AttentionModule.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_datas_steps</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass of the AttentionModule.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        batch_datas_steps : torch.Tensor</span>
<span class="sd">            Batched input tensor with dimensions (batch_size, step_size, fea_size).</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Output tensor after encoding, applying attention, residual blocks, and decoding.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">batch_datas_steps</span> <span class="o">=</span> <span class="n">batch_datas_steps</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">step_size</span><span class="p">,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">fea_size</span><span class="p">]</span>
        <span class="n">encoder_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">batch_datas_steps</span><span class="p">)</span>

        <span class="n">encoder_output</span> <span class="o">=</span> <span class="n">encoder_output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">encoder_output</span>

        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">l_list</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">+</span> <span class="n">output</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">output</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span></div></div>


<div class="viewcode-block" id="LambdaRankLoss"><a class="viewcode-back" href="../case_study/tlp/tlp_fine_tune.html#tlp_fine_tune.LambdaRankLoss">[docs]</a><span class="k">class</span> <span class="nc">LambdaRankLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Defines the LambdaRank loss function, which is commonly used for learning to rank tasks.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    device : torch.device</span>
<span class="sd">        The device on which calculations are performed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>

<div class="viewcode-block" id="LambdaRankLoss.lamdbaRank_scheme"><a class="viewcode-back" href="../case_study/tlp/tlp_fine_tune.html#tlp_fine_tune.LambdaRankLoss.lamdbaRank_scheme">[docs]</a>    <span class="k">def</span> <span class="nf">lamdbaRank_scheme</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates weights for pairs of items based on their relevance scores and positions.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        G : torch.Tensor</span>
<span class="sd">            Gain matrix for relevance scores.</span>
<span class="sd">        D : torch.Tensor</span>
<span class="sd">            Discount matrix for positional discounts.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Weight matrix based on LambdaRank scheme.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">D</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">],</span> <span class="o">-</span><span class="mf">1.</span><span class="p">)</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">D</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="o">-</span><span class="mf">1.</span><span class="p">))</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span>
            <span class="n">G</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">G</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:])</span></div>

<div class="viewcode-block" id="LambdaRankLoss.forward"><a class="viewcode-back" href="../case_study/tlp/tlp_fine_tune.html#tlp_fine_tune.LambdaRankLoss.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the LambdaRank loss between predicted and actual relevance scores.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        preds : torch.Tensor</span>
<span class="sd">            Predicted relevance scores.</span>
<span class="sd">        labels : torch.Tensor</span>
<span class="sd">            Actual relevance scores.</span>
<span class="sd">        k : int, optional</span>
<span class="sd">            Rank cutoff parameter.</span>
<span class="sd">        eps : float</span>
<span class="sd">            Small constant for numerical stability.</span>
<span class="sd">        mu : float</span>
<span class="sd">            Hyperparameter for LambdaRank scheme.</span>
<span class="sd">        sigma : float</span>
<span class="sd">            Hyperparameter for sigmoid scaling in probability calculation.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The computed LambdaRank loss.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">preds</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">y_true</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

        <span class="n">y_pred_sorted</span><span class="p">,</span> <span class="n">indices_pred</span> <span class="o">=</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y_true_sorted</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">y_true</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">true_sorted_by_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">indices_pred</span><span class="p">)</span>
        <span class="n">true_diffs</span> <span class="o">=</span> <span class="n">true_sorted_by_preds</span><span class="p">[:,</span> <span class="p">:,</span>
                                          <span class="kc">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">true_sorted_by_preds</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">padded_pairs_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">true_diffs</span><span class="p">)</span>

        <span class="n">padded_pairs_mask</span> <span class="o">=</span> <span class="n">padded_pairs_mask</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">true_diffs</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">ndcg_at_k_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">ndcg_at_k_mask</span><span class="p">[:</span><span class="n">k</span><span class="p">,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="n">true_sorted_by_preds</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
        <span class="n">y_true_sorted</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>

        <span class="n">pos_idxs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">pos_idxs</span><span class="o">.</span><span class="n">float</span><span class="p">())[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">maxDCGs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
            <span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">y_true_sorted</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">D</span><span class="p">)[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
        <span class="n">G</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">true_sorted_by_preds</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">maxDCGs</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>

        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lamdbaRank_scheme</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">true_sorted_by_preds</span><span class="p">)</span>

        <span class="n">scores_diffs</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">y_pred_sorted</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">y_pred_sorted</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=-</span><span class="mf">1e8</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1e8</span><span class="p">)</span>
        <span class="n">scores_diffs</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">scores_diffs</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">0.</span>
        <span class="n">weighted_probas</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span>
            <span class="n">sigma</span> <span class="o">*</span> <span class="n">scores_diffs</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span> <span class="o">**</span> <span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">weighted_probas</span><span class="p">)</span>
        <span class="n">masked_losses</span> <span class="o">=</span> <span class="n">losses</span><span class="p">[</span><span class="n">padded_pairs_mask</span> <span class="o">&amp;</span> <span class="n">ndcg_at_k_mask</span><span class="p">]</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">masked_losses</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span></div></div>


<div class="viewcode-block" id="SegmentDataLoader"><a class="viewcode-back" href="../case_study/tlp/tlp_fine_tune.html#tlp_fine_tune.SegmentDataLoader">[docs]</a><span class="k">class</span> <span class="nc">SegmentDataLoader</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Custom data loader for segmented data with variable batch sizes and shuffling options.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    shuffle : bool</span>
<span class="sd">        Whether to shuffle the data on each iteration.</span>
<span class="sd">    batch_size : int</span>
<span class="sd">        Number of samples in each batch.</span>
<span class="sd">    datas_steps : torch.Tensor</span>
<span class="sd">        Tensor of data sequences (steps) to be used in training/validation.</span>
<span class="sd">    labels : torch.Tensor</span>
<span class="sd">        Tensor of labels corresponding to the data steps.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">dataset</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span> <span class="o">=</span> <span class="n">shuffle</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iter_order</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointer</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">datas_steps</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">min_latency</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">data_idx</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
            <span class="n">datas_step</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">min_lat</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">datas_steps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">datas_step</span><span class="p">)</span>
            <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
            <span class="n">min_latency</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">min_lat</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">datas_steps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">datas_steps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_latency</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">min_latency</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">number</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">datas_steps</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">iter_order</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">number</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">iter_order</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">number</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pointer</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="fm">__next__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointer</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">number</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">StopIteration</span>

        <span class="n">batch_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter_order</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pointer</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointer</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pointer</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fetch_indices</span><span class="p">(</span><span class="n">batch_indices</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_fetch_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>

        <span class="n">batch_datas_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">datas_steps</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
        <span class="n">batch_datas_steps</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_sequence</span><span class="p">(</span>
            <span class="n">batch_datas_steps</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">batch_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">batch_datas_steps</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">number</span></div>


<div class="viewcode-block" id="load_datas"><a class="viewcode-back" href="../case_study/tlp/tlp_fine_tune.html#tlp_fine_tune.load_datas">[docs]</a><span class="k">def</span> <span class="nf">load_datas</span><span class="p">(</span><span class="n">datasets_global</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits the dataset into training and validation sets, and initializes data loaders for both.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    datasets_global : list</span>
<span class="sd">        List of all data samples to be used in training and validation.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    tuple</span>
<span class="sd">        Tuple containing training and validation data loaders.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">datasets</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">datasets_global</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
    <span class="n">train_len</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_cnt</span> <span class="o">*</span> <span class="mi">1000</span> <span class="o">*</span> <span class="mf">0.9</span><span class="p">)</span>

    <span class="n">perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">datasets</span><span class="p">))</span>
    <span class="n">train_indices</span><span class="p">,</span> <span class="n">val_indices</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[:</span><span class="n">train_len</span><span class="p">],</span> <span class="n">perm</span><span class="p">[</span><span class="n">train_len</span><span class="p">:</span><span class="n">args</span><span class="o">.</span><span class="n">data_cnt</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">]</span>

    <span class="n">train_datas</span><span class="p">,</span> <span class="n">val_datas</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="n">train_indices</span><span class="p">],</span> <span class="n">datasets</span><span class="p">[</span><span class="n">val_indices</span><span class="p">]</span>

    <span class="n">n_gpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">SegmentDataLoader</span><span class="p">(</span>
        <span class="n">train_datas</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">train_size_per_gpu</span><span class="o">*</span><span class="n">n_gpu</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">val_dataloader</span> <span class="o">=</span> <span class="n">SegmentDataLoader</span><span class="p">(</span>
        <span class="n">val_datas</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">val_size_per_gpu</span><span class="o">*</span><span class="n">n_gpu</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span></div>


<div class="viewcode-block" id="validate"><a class="viewcode-back" href="../case_study/tlp/tlp_fine_tune.html#tlp_fine_tune.validate">[docs]</a><span class="k">def</span> <span class="nf">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluates the model on the validation set and computes the total validation loss.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    model : nn.Module</span>
<span class="sd">        The model to be validated.</span>
<span class="sd">    valid_loader : SegmentDataLoader</span>
<span class="sd">        Data loader for the validation set.</span>
<span class="sd">    loss_func : nn.Module</span>
<span class="sd">        Loss function to use for validation.</span>
<span class="sd">    device : torch.device</span>
<span class="sd">        The device on which the model and data are loaded.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    float</span>
<span class="sd">        Total validation loss.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">valid_losses</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">batch_datas_steps</span><span class="p">,</span> <span class="n">batch_labels</span> <span class="ow">in</span> <span class="n">valid_loader</span><span class="p">:</span>
        <span class="n">batch_datas_steps</span> <span class="o">=</span> <span class="n">batch_datas_steps</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">batch_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch_datas_steps</span><span class="p">)</span>
        <span class="n">valid_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">valid_losses</span><span class="p">)</span></div>


<div class="viewcode-block" id="train"><a class="viewcode-back" href="../case_study/tlp/tlp_fine_tune.html#tlp_fine_tune.train">[docs]</a><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Trains the model over multiple epochs and saves the model at the end of each epoch.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    train_loader : SegmentDataLoader</span>
<span class="sd">        Data loader for the training set.</span>
<span class="sd">    val_dataloader : SegmentDataLoader</span>
<span class="sd">        Data loader for the validation set.</span>
<span class="sd">    device : torch.device</span>
<span class="sd">        The device on which the model and data are loaded.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># n_epoch = 50</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">attention_class</span> <span class="o">==</span> <span class="s1">&#39;default&#39;</span><span class="p">:</span>
        <span class="n">args</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span>
        <span class="n">args</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">pre_train_model</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">pre_train_model</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">net</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">net</span> <span class="o">=</span> <span class="n">AttentionModule</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">net</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>
    

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">rank_mse</span> <span class="o">==</span> <span class="s1">&#39;rank&#39;</span><span class="p">:</span>
        <span class="n">loss_func</span> <span class="o">=</span> <span class="n">LambdaRankLoss</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

    <span class="n">n_epoch</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_epoch</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;default&#39;</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">)</span>
        <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="n">n_epoch</span> <span class="o">//</span> <span class="mi">3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;decrease_per_17_0.8&#39;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;optimizer&#39;</span><span class="p">,</span> <span class="s1">&#39;decrease_per_17_0.8&#39;</span><span class="p">)</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">)</span>
        <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="n">n_epoch</span> <span class="o">//</span> <span class="mi">3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;decrease_per_17_0.5&#39;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;optimizer&#39;</span><span class="p">,</span> <span class="s1">&#39;decrease_per_17_0.5&#39;</span><span class="p">)</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">)</span>
        <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="n">n_epoch</span> <span class="o">//</span> <span class="mi">3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;decrease_per_12_0.5&#39;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;optimizer&#39;</span><span class="p">,</span> <span class="s1">&#39;decrease_per_12_0.5&#39;</span><span class="p">)</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">)</span>
        <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="n">n_epoch</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;decrease_per_10_0.5&#39;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;optimizer&#39;</span><span class="p">,</span> <span class="s1">&#39;decrease_per_10_0.5&#39;</span><span class="p">)</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">)</span>
        <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="n">n_epoch</span> <span class="o">//</span> <span class="mi">5</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;decrease_per_17_0.5_no_decay&#39;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;optimizer&#39;</span><span class="p">,</span> <span class="s1">&#39;decrease_per_17_0.5&#39;</span><span class="p">)</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>
        <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="n">n_epoch</span> <span class="o">//</span> <span class="mi">3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="n">train_loss</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;start train...&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_dataloader</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epoch</span><span class="p">):</span>
        <span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_datas_steps</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
            <span class="n">batch_datas_steps</span> <span class="o">=</span> <span class="n">batch_datas_steps</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">batch_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">batch_datas_steps</span><span class="p">),</span> <span class="n">batch_labels</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mf">0.5</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">train_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">tic</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">==</span> <span class="n">n_epoch</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">or</span> <span class="kc">True</span><span class="p">:</span>

            <span class="n">valid_loss</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">,</span>
                                  <span class="n">loss_func</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="n">loss_msg</span> <span class="o">=</span> <span class="s2">&quot;Train Loss: </span><span class="si">%.4f</span><span class="se">\t</span><span class="s2">Valid Loss: </span><span class="si">%.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span>
                <span class="n">train_loss</span><span class="p">,</span> <span class="n">valid_loss</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Epoch: </span><span class="si">%d</span><span class="se">\t</span><span class="s2">Batch: </span><span class="si">%d</span><span class="se">\t</span><span class="si">%s</span><span class="se">\t</span><span class="s2">Train Speed: </span><span class="si">%.0f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span>
                <span class="n">epoch</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">loss_msg</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">/</span> <span class="n">train_time</span><span class="p">,))</span>

        <span class="n">model_save_file_name</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">/tlp_model_</span><span class="si">%d</span><span class="s1">.pkl&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">save_folder</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_save_file_name</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">f</span><span class="p">)</span>
        <span class="n">net</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></div>


<div class="viewcode-block" id="set_seed"><a class="viewcode-back" href="../case_study/tlp/tlp_fine_tune.html#tlp_fine_tune.set_seed">[docs]</a><span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets random seed for reproducibility across numpy, Python, and PyTorch libraries.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    seed : int</span>
<span class="sd">        The seed value to set.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span></div>


<span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--save_folder&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--cuda&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--dataset&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;/home/huanting/PROM/examples/case_study/tlp/scripts/data_model/bert_base_train_and_val.pkl&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--lr&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">7e-4</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--weight_decay&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--rank_mse&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;rank&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--optimizer&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;default&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--attention_head&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--attention_class&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;default&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--step_size&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--fea_size&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">22</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--res_block_cnt&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--self_sup_model&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--pre_train_model&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--data_cnt&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>  <span class="c1"># data_cnt * 1000</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--train_size_per_gpu&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--val_size_per_gpu&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--n_epoch&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">save_folder</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;create folder&#39;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">save_folder</span><span class="p">)</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">save_folder</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;load data...&#39;</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">datasets_global</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;load pkl done.&#39;</span><span class="p">)</span>
    <span class="n">datas</span> <span class="o">=</span> <span class="n">load_datas</span><span class="p">(</span><span class="n">datasets_global</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;create dataloader done.&#39;</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">datasets_global</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;load data done.&#39;</span><span class="p">)</span>
    <span class="n">train</span><span class="p">(</span><span class="o">*</span><span class="n">datas</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">cuda</span><span class="p">)</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, _.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>